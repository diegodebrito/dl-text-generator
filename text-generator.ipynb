{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-31T00:36:02.169004Z",
     "start_time": "2020-03-31T00:36:02.002909Z"
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-31T00:36:02.180916Z",
     "start_time": "2020-03-31T00:36:02.170905Z"
    }
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Input, Embedding, Dense, LSTM, Bidirectional\n",
    "from tensorflow.keras.layers import SpatialDropout1D, concatenate\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import LearningRateScheduler\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer, text_to_word_sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Main Reference:\n",
    "https://github.com/minimaxir/textgenrnn/tree/master/textgenrnn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Attention Weighted Average Layer\n",
    "https://gist.github.com/thomwolf/e309e779a08c1ba899514d44355cd6df  \n",
    "https://github.com/minimaxir/textgenrnn/blob/1a271addb5894aaa485d3f23663a9afb813f7692/textgenrnn/AttentionWeightedAverage.py#L6\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-31T00:37:28.018942Z",
     "start_time": "2020-03-31T00:37:28.002941Z"
    }
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Layer, InputSpec\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras import initializers\n",
    "\n",
    "\n",
    "class AttentionWeightedAverage(Layer):\n",
    "    \"\"\"\n",
    "    Computes a weighted average of the different channels across timesteps.\n",
    "    Uses 1 parameter pr. channel to compute the attention value for\n",
    "    a single timestep.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, return_attention=False, **kwargs):\n",
    "        self.init = initializers.get('uniform')\n",
    "        self.supports_masking = True\n",
    "        self.return_attention = return_attention\n",
    "        super(AttentionWeightedAverage, self).__init__(** kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.input_spec = [InputSpec(ndim=3)]\n",
    "        assert len(input_shape) == 3\n",
    "\n",
    "        self.W = self.add_weight(shape=(input_shape[2], 1),\n",
    "                                 name='{}_W'.format(self.name),\n",
    "                                 trainable=True,\n",
    "                                 initializer=self.init)\n",
    "        super(AttentionWeightedAverage, self).build(input_shape)\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        # computes a probability distribution over the timesteps\n",
    "        # uses 'max trick' for numerical stability\n",
    "        # reshape is done to avoid issue with Tensorflow\n",
    "        # and 1-dimensional weights\n",
    "        logits = K.dot(x, self.W)\n",
    "        x_shape = K.shape(x)\n",
    "        logits = K.reshape(logits, (x_shape[0], x_shape[1]))\n",
    "        ai = K.exp(logits - K.max(logits, axis=-1, keepdims=True))\n",
    "\n",
    "        # masked timesteps have zero weight\n",
    "        if mask is not None:\n",
    "            mask = K.cast(mask, K.floatx())\n",
    "            ai = ai * mask\n",
    "        att_weights = ai / (K.sum(ai, axis=1, keepdims=True) + K.epsilon())\n",
    "        weighted_input = x * K.expand_dims(att_weights)\n",
    "        result = K.sum(weighted_input, axis=1)\n",
    "        if self.return_attention:\n",
    "            return [result, att_weights]\n",
    "        return result\n",
    "\n",
    "    def get_output_shape_for(self, input_shape):\n",
    "        return self.compute_output_shape(input_shape)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        output_len = input_shape[2]\n",
    "        if self.return_attention:\n",
    "            return [(input_shape[0], output_len), (input_shape[0],\n",
    "                                                   input_shape[1])]\n",
    "        return (input_shape[0], output_len)\n",
    "\n",
    "    def compute_mask(self, input, input_mask=None):\n",
    "        if isinstance(input_mask, list):\n",
    "            return [None] * len(input_mask)\n",
    "        else:\n",
    "            return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-31T00:35:38.361820Z",
     "start_time": "2020-03-31T00:35:38.356820Z"
    }
   },
   "outputs": [],
   "source": [
    "def create_dataset(data, n_step, batch_size):\n",
    "    \n",
    "    window_length = n_step + 1\n",
    "    dataset = tf.data.Dataset.from_tensor_slices(data)\n",
    "    dataset = dataset.window(window_length, shift=1, drop_remainder=True)\n",
    "    dataset = dataset.flat_map(lambda window: window.batch(window_length))\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    dataset = dataset.map(lambda windows: (windows[:, :-1], windows[:, -1]))\n",
    "    dataset = dataset.prefetch(1)\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Character-level Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Textgenrnn's Weights and Vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading Textgenrnn Vocabulary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-30T23:26:58.056679Z",
     "start_time": "2020-03-30T23:26:58.050708Z"
    }
   },
   "outputs": [],
   "source": [
    "vocab_path = './textgenrnn_vocab.json'\n",
    "with open(vocab_path, 'r', encoding='utf8', errors='ignore') as json_file:\n",
    "    vocab = json.load(json_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reverse vocabulary and number of classes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-30T23:26:58.062769Z",
     "start_time": "2020-03-30T23:26:58.058676Z"
    }
   },
   "outputs": [],
   "source": [
    "indices_char = dict((vocab[c], c) for c in vocab)\n",
    "num_classes = len(vocab) + 1 # To allow for padding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model characteristics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-30T23:26:58.070743Z",
     "start_time": "2020-03-30T23:26:58.064734Z"
    }
   },
   "outputs": [],
   "source": [
    "MAX_LENGTH = 40\n",
    "DIM_EMBEDDINGS = 100\n",
    "DROPOUT = 0.0\n",
    "RNN_LAYERS = 2\n",
    "RNN_SIZE = 128"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Building the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-30T23:26:58.819179Z",
     "start_time": "2020-03-30T23:26:58.072731Z"
    }
   },
   "outputs": [],
   "source": [
    "input_layer = Input(shape=(MAX_LENGTH, ), name='input')\n",
    "\n",
    "# Try a different embedding?\n",
    "embedding_layer = Embedding(num_classes, DIM_EMBEDDINGS, \n",
    "                            input_length = MAX_LENGTH,\n",
    "                            name='embedding')(input_layer)\n",
    "\n",
    "# SpatialDropout after the Embedding layer drops one entire row/feature!\n",
    "if DROPOUT > 0.0:\n",
    "    embedding_layer = SpatialDropout1D(DROPOUT, \n",
    "                                       name='regularized_embedding')(embedding_layer)\n",
    "    \n",
    "# Try changing to bidirectional\n",
    "rnn_layers_list = []\n",
    "for i in range(RNN_LAYERS):\n",
    "    prev_layer = embedding_layer if i==0 else rnn_layers_list[-1]\n",
    "    next_layer = LSTM(RNN_SIZE, return_sequences=True, name = f\"lstm{i}\")\n",
    "    rnn_layers_list.append(next_layer(prev_layer))\n",
    "    \n",
    "# Concatenating the layers created so far\n",
    "seq = concatenate([embedding_layer] + rnn_layers_list)\n",
    "\n",
    "# Attention layer (see references above)\n",
    "attention = AttentionWeightedAverage(name='attention_layer')(seq)\n",
    "\n",
    "# Output\n",
    "output = Dense(num_classes, name='output', activation='softmax')(attention)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compiling:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-30T23:26:58.850158Z",
     "start_time": "2020-03-30T23:26:58.823164Z"
    }
   },
   "outputs": [],
   "source": [
    "model = Model(inputs=[input_layer], outputs=[output])\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading Pre-Trained Weights:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-30T23:26:58.868157Z",
     "start_time": "2020-03-30T23:26:58.851157Z"
    }
   },
   "outputs": [],
   "source": [
    "model.load_weights('./textgenrnn_weights.hdf5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's generate some predictions. Ww need a function to generate the id for the next word based on the current text being used. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-30T23:04:43.919421Z",
     "start_time": "2020-03-30T23:04:43.913417Z"
    }
   },
   "outputs": [],
   "source": [
    "def next_letter_id(text, vocab, maxlen, model, temperature=0.5):\n",
    "    \n",
    "    # Encode the text using the vocabulary and pad sequences to maxlen\n",
    "    encoded = np.array([vocab.get(x, 0) for x in text])\n",
    "    encoded_padded = sequence.pad_sequences([encoded], maxlen=maxlen)\n",
    "    \n",
    "    # Compute model predictions\n",
    "    preds = model.predict(encoded_padded)[0]\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    \n",
    "    # Now let's calculate the probabilities. The temperature variable\n",
    "    # controls how much we let the model decide\n",
    "    preds = np.log(preds + K.epsilon()) / temperature\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    probas = np.random.multinomial(1, preds, 1)\n",
    "    \n",
    "    # Getting the index for the most likely character\n",
    "    index = np.argmax(probas)\n",
    "    \n",
    "    # We get the second most likely character when we select padding character\n",
    "    if index == 0:\n",
    "        index = np.argsort(preds)[-2]\n",
    "\n",
    "    return index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's start with a text and see what it generates:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-30T23:04:43.926417Z",
     "start_time": "2020-03-30T23:04:43.921419Z"
    }
   },
   "outputs": [],
   "source": [
    "def generate_text(text, size=150, temperature=0.75):\n",
    "    for _ in range(size):\n",
    "        text += indices_char[next_letter_id(text, vocab, MAX_LENGTH, \n",
    "                                            model, \n",
    "                                            temperature=temperature)]\n",
    "    print(''.join(text))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-30T23:04:49.202715Z",
     "start_time": "2020-03-30T23:04:43.929419Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Today 4.0<s>shift in the third windown things you enjoy when I see them in the desperate elephants?<s>da and PS4 will have an online minute depressed on the dr\n"
     ]
    }
   ],
   "source": [
    "text = ['T', 'o', 'd', 'a', 'y']\n",
    "generate_text(text, temperature=0.6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to the documentation, the original model was trained on hundreds of thousands of text documents from Reddit submissions, using a diverse variety of subreddits. The results are pretty interesting, using right grammar and sentence structure, even if the meaning is not necessarily accurate. You can play with size and temperature to get different results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-Tuning: War and Peace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The textgenrnn package is intended to be used with texts given by the user. According to the documentation, the approach is to train the model on the new text starting with the pre-trained weights used above. Although this is not the usual approach when using transfer learning, let's start with it. First, we need to load and parse out text accordingly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-30T23:27:07.465826Z",
     "start_time": "2020-03-30T23:27:07.455922Z"
    }
   },
   "outputs": [],
   "source": [
    "filepath = './war_peace.txt'\n",
    "with open(filepath, encoding='UTF-8') as f:\n",
    "    war = f.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenizing using the vocabulary from Textgenrnn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-30T23:27:10.517715Z",
     "start_time": "2020-03-30T23:27:09.827640Z"
    }
   },
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(filters = '', lower = False, char_level=True)\n",
    "tokenizer.word_index = vocab\n",
    "[encoded] = np.array(tokenizer.texts_to_sequences([war]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-30T23:27:10.523073Z",
     "start_time": "2020-03-30T23:27:10.518711Z"
    }
   },
   "outputs": [],
   "source": [
    "num_classes = len(vocab) + 1\n",
    "dataset_size = len(encoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train and Validation Data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-30T23:27:10.533668Z",
     "start_time": "2020-03-30T23:27:10.525077Z"
    }
   },
   "outputs": [],
   "source": [
    "train_size = dataset_size * 90 // 100\n",
    "train_data = encoded[:train_size]\n",
    "val_data = encoded[train_size:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating the datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-30T23:27:26.561597Z",
     "start_time": "2020-03-30T23:27:26.557984Z"
    }
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-30T23:27:26.654310Z",
     "start_time": "2020-03-30T23:27:26.575589Z"
    }
   },
   "outputs": [],
   "source": [
    "train_dataset = create_dataset(train_data, MAX_LENGTH, BATCH_SIZE)\n",
    "val_dataset = create_dataset(val_data, MAX_LENGTH, BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training the model using a variable learning rate this time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-30T23:12:38.481479Z",
     "start_time": "2020-03-30T23:12:38.475476Z"
    }
   },
   "outputs": [],
   "source": [
    "def lr_linear_decay(epoch):\n",
    "            return (base_lr * (1 - (epoch / num_epochs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-30T23:13:37.830932Z",
     "start_time": "2020-03-30T23:13:37.827970Z"
    }
   },
   "outputs": [],
   "source": [
    "num_epochs = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(train_dataset, \n",
    "          validation_data=val_dataset, \n",
    "          epochs = num_epochs,\n",
    "          callbacks=[LearningRateScheduler(lr_linear_decay)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word-Level Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-31T00:34:55.182610Z",
     "start_time": "2020-03-31T00:34:52.035194Z"
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer, text_to_word_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-31T00:35:10.591613Z",
     "start_time": "2020-03-31T00:35:10.580649Z"
    }
   },
   "outputs": [],
   "source": [
    "filepath = './war_peace.txt'\n",
    "with open(filepath, encoding='UTF-8') as f:\n",
    "    war = f.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting the list of words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-31T00:35:13.216984Z",
     "start_time": "2020-03-31T00:35:12.641883Z"
    }
   },
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/a/3645946/9314418\n",
    "punct = '!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~\\\\n\\\\t\\'‘’“”’–—…'\n",
    "text = re.sub('([{}])'.format(punct), r' \\1 ', war)\n",
    "text = re.sub(' {2,}', ' ', text)\n",
    "text = text_to_word_sequence(text, filters='')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fitting a tokenizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-31T00:35:16.126428Z",
     "start_time": "2020-03-31T00:35:14.334332Z"
    }
   },
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(filters = '', lower = True, \n",
    "                      char_level = False)\n",
    "tokenizer.fit_on_texts(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's limit the vocabulary to the most common 10000 words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-31T00:35:18.536010Z",
     "start_time": "2020-03-31T00:35:18.530017Z"
    }
   },
   "outputs": [],
   "source": [
    "max_words = 10000\n",
    "vocab = {k:v for (k,v) in tokenizer.word_index.items() if v<=10000}\n",
    "tokenizer.word_index = vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now tokenizing our words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-31T00:35:19.682758Z",
     "start_time": "2020-03-31T00:35:19.400660Z"
    }
   },
   "outputs": [],
   "source": [
    "[encoded] = np.array(tokenizer.texts_to_sequences([text]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the words are encoded, preparing the data is similar to what we did for the character-level models. In this case, we can use a smaller MAX_LENGTH though. Defining constants:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-31T00:35:21.405336Z",
     "start_time": "2020-03-31T00:35:21.401335Z"
    }
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128\n",
    "num_classes = len(vocab) + 1\n",
    "dataset_size = len(encoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train-validation split:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-31T00:35:24.510069Z",
     "start_time": "2020-03-31T00:35:24.505063Z"
    }
   },
   "outputs": [],
   "source": [
    "train_size = dataset_size * 90 // 100\n",
    "train_data = encoded[:train_size]\n",
    "val_data = encoded[train_size:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-31T00:36:07.679051Z",
     "start_time": "2020-03-31T00:36:07.581042Z"
    }
   },
   "outputs": [],
   "source": [
    "train_dataset = create_dataset(train_data, MAX_LENGTH, BATCH_SIZE)\n",
    "val_dataset = create_dataset(val_data, MAX_LENGTH, BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-31T00:37:22.981151Z",
     "start_time": "2020-03-31T00:37:22.977153Z"
    }
   },
   "outputs": [],
   "source": [
    "MAX_LENGTH = 10\n",
    "DIM_EMBEDDINGS = 100\n",
    "DROPOUT = 0.0\n",
    "RNN_LAYERS = 2\n",
    "RNN_SIZE = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-31T00:37:33.032530Z",
     "start_time": "2020-03-31T00:37:32.393481Z"
    }
   },
   "outputs": [],
   "source": [
    "input_layer = Input(shape=(MAX_LENGTH, ), name='input')\n",
    "\n",
    "# Try a different embedding?\n",
    "embedding_layer = Embedding(num_classes, DIM_EMBEDDINGS, \n",
    "                            input_length = MAX_LENGTH,\n",
    "                            name='embedding')(input_layer)\n",
    "\n",
    "# SpatialDropout after the Embedding layer drops one entire row/feature!\n",
    "if DROPOUT > 0.0:\n",
    "    embedding_layer = SpatialDropout1D(DROPOUT, \n",
    "                                       name='regularized_embedding')(embedding_layer)\n",
    "    \n",
    "# Try changing to bidirectional\n",
    "rnn_layers_list = []\n",
    "for i in range(RNN_LAYERS):\n",
    "    prev_layer = embedding_layer if i==0 else rnn_layers_list[-1]\n",
    "    next_layer = LSTM(RNN_SIZE, return_sequences=True, name = f\"lstm{i}\")\n",
    "    rnn_layers_list.append(next_layer(prev_layer))\n",
    "    \n",
    "# Concatenating the layers created so far\n",
    "seq = concatenate([embedding_layer] + rnn_layers_list)\n",
    "\n",
    "# Attention layer (see references above)\n",
    "attention = AttentionWeightedAverage(name='attention_layer')(seq)\n",
    "\n",
    "# Output\n",
    "output = Dense(num_classes, name='output', activation='softmax')(attention)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-31T00:37:35.085705Z",
     "start_time": "2020-03-31T00:37:35.055676Z"
    }
   },
   "outputs": [],
   "source": [
    "model = Model(inputs=[input_layer], outputs=[output])\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-03-31T00:29:21.310Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "    140/Unknown - 24s 169ms/step - loss: 6.5599"
     ]
    }
   ],
   "source": [
    "model.fit(train_dataset, \n",
    "          validation_data=val_dataset, \n",
    "          epochs = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
